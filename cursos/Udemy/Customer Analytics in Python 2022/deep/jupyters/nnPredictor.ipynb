{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediciendo la compra de audiolibros\n",
    "\n",
    "En este ejemplo, lo que haremos es utilizar una red neuronal para predecir si una persona comprará o no un audiolibro. Para esto, usaremos un set de datos que contiene información de diferentes clientes, entre esta info podemos encontrar, la cantidad de veces que compro, la cantidad de veces que accedió al sitio, si dejo o no dejo una reseña, qué puntaje colocó en la reseña, entre otros.\n",
    "\n",
    "Utilizaremos [Tensorflow](https://www.tensorflow.org/?gclid=Cj0KCQiAwJWdBhCYARIsAJc4idBKBFxY8qIb2YFIXTld4WhSSr7yb-b-0EUxf2CCWwKaXOUG44Wpt6IaAueKEALw_wcB) para crear un modelo de red neuronal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importamos librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample, shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.730e+02, 2.160e+03, 2.160e+03, 1.013e+01, 1.013e+01, 0.000e+00,\n",
       "       8.910e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 1.000e+00])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = np.loadtxt('datasets/audiobooks_data.csv', delimiter=\",\")\n",
    "raw_data[0,:].round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesando datos\n",
    "\n",
    "Antes que nada, vamos a preprocesar los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe\n",
    "\n",
    "Crearemos un dataframe con los datos cargados sólo para hacer un rápido análisis. Sin embargo, para entrenar y evaluar la red neuronal solo usaremos *raw_data*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Book length (mins)_overall</th>\n",
       "      <th>Book length (mins)_avg</th>\n",
       "      <th>Price_overall</th>\n",
       "      <th>Price_avg</th>\n",
       "      <th>Review</th>\n",
       "      <th>Review 10/10</th>\n",
       "      <th>Completion</th>\n",
       "      <th>Minutes Listened</th>\n",
       "      <th>Support Requests</th>\n",
       "      <th>Last visited minus Purchase date</th>\n",
       "      <th>Targets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>873.0</td>\n",
       "      <td>2160.0</td>\n",
       "      <td>2160.0</td>\n",
       "      <td>10.13</td>\n",
       "      <td>10.13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.91</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>611.0</td>\n",
       "      <td>1404.0</td>\n",
       "      <td>2808.0</td>\n",
       "      <td>6.66</td>\n",
       "      <td>13.33</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>705.0</td>\n",
       "      <td>324.0</td>\n",
       "      <td>324.0</td>\n",
       "      <td>10.13</td>\n",
       "      <td>10.13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>334.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>391.0</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>15.31</td>\n",
       "      <td>15.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>819.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>1296.0</td>\n",
       "      <td>7.11</td>\n",
       "      <td>21.33</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  Book length (mins)_overall  Book length (mins)_avg  Price_overall  \\\n",
       "0  873.0                      2160.0                  2160.0          10.13   \n",
       "1  611.0                      1404.0                  2808.0           6.66   \n",
       "2  705.0                       324.0                   324.0          10.13   \n",
       "3  391.0                      1620.0                  1620.0          15.31   \n",
       "4  819.0                       432.0                  1296.0           7.11   \n",
       "\n",
       "   Price_avg  Review  Review 10/10  Completion  Minutes Listened  \\\n",
       "0      10.13     0.0          8.91         0.0               0.0   \n",
       "1      13.33     1.0          6.50         0.0               0.0   \n",
       "2      10.13     1.0          9.00         0.0               0.0   \n",
       "3      15.31     0.0          9.00         0.0               0.0   \n",
       "4      21.33     1.0          9.00         0.0               0.0   \n",
       "\n",
       "   Support Requests  Last visited minus Purchase date  Targets  \n",
       "0               0.0                               0.0      1.0  \n",
       "1               0.0                             182.0      1.0  \n",
       "2               1.0                             334.0      1.0  \n",
       "3               0.0                             183.0      1.0  \n",
       "4               0.0                               0.0      1.0  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columnas = [\"ID\",\"Book length (mins)_overall\",\"Book length (mins)_avg\", \"Price_overall\",\n",
    "            \"Price_avg\", \"Review\", \"Review 10/10\", \"Completion\", \"Minutes Listened\",\n",
    "            \"Support Requests\", \"Last visited minus Purchase date\", \"Targets\"]\n",
    "\n",
    "raw_df = pd.DataFrame(raw_data, columns=columnas)\n",
    "# raw_df.describe().round(2).head()\n",
    "raw_df.head()\n",
    "# raw_df[raw_df[\"ID\"]==994]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book length (mins)_overall</th>\n",
       "      <th>Book length (mins)_avg</th>\n",
       "      <th>Price_overall</th>\n",
       "      <th>Price_avg</th>\n",
       "      <th>Review</th>\n",
       "      <th>Review 10/10</th>\n",
       "      <th>Completion</th>\n",
       "      <th>Minutes Listened</th>\n",
       "      <th>Support Requests</th>\n",
       "      <th>Last visited minus Purchase date</th>\n",
       "      <th>Targets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14084.000</td>\n",
       "      <td>14084.000</td>\n",
       "      <td>14084.000</td>\n",
       "      <td>14084.000</td>\n",
       "      <td>14084.000</td>\n",
       "      <td>14084.000</td>\n",
       "      <td>14084.000</td>\n",
       "      <td>14084.000</td>\n",
       "      <td>14084.000</td>\n",
       "      <td>14084.000</td>\n",
       "      <td>14084.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1591.282</td>\n",
       "      <td>1678.609</td>\n",
       "      <td>7.104</td>\n",
       "      <td>7.544</td>\n",
       "      <td>0.161</td>\n",
       "      <td>8.910</td>\n",
       "      <td>0.126</td>\n",
       "      <td>118.587</td>\n",
       "      <td>0.070</td>\n",
       "      <td>61.935</td>\n",
       "      <td>0.159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>504.341</td>\n",
       "      <td>654.839</td>\n",
       "      <td>4.932</td>\n",
       "      <td>5.560</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.241</td>\n",
       "      <td>268.732</td>\n",
       "      <td>0.472</td>\n",
       "      <td>88.208</td>\n",
       "      <td>0.366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>216.000</td>\n",
       "      <td>216.000</td>\n",
       "      <td>3.860</td>\n",
       "      <td>3.860</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1188.000</td>\n",
       "      <td>1188.000</td>\n",
       "      <td>5.330</td>\n",
       "      <td>5.330</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.910</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1620.000</td>\n",
       "      <td>1620.000</td>\n",
       "      <td>5.950</td>\n",
       "      <td>6.070</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.910</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2160.000</td>\n",
       "      <td>2160.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.910</td>\n",
       "      <td>0.130</td>\n",
       "      <td>64.800</td>\n",
       "      <td>0.000</td>\n",
       "      <td>105.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2160.000</td>\n",
       "      <td>7020.000</td>\n",
       "      <td>130.940</td>\n",
       "      <td>130.940</td>\n",
       "      <td>1.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2116.800</td>\n",
       "      <td>30.000</td>\n",
       "      <td>464.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Book length (mins)_overall  Book length (mins)_avg  Price_overall  \\\n",
       "count                   14084.000               14084.000      14084.000   \n",
       "mean                     1591.282                1678.609          7.104   \n",
       "std                       504.341                 654.839          4.932   \n",
       "min                       216.000                 216.000          3.860   \n",
       "25%                      1188.000                1188.000          5.330   \n",
       "50%                      1620.000                1620.000          5.950   \n",
       "75%                      2160.000                2160.000          8.000   \n",
       "max                      2160.000                7020.000        130.940   \n",
       "\n",
       "       Price_avg     Review  Review 10/10  Completion  Minutes Listened  \\\n",
       "count  14084.000  14084.000     14084.000   14084.000         14084.000   \n",
       "mean       7.544      0.161         8.910       0.126           118.587   \n",
       "std        5.560      0.367         0.643       0.241           268.732   \n",
       "min        3.860      0.000         1.000       0.000             0.000   \n",
       "25%        5.330      0.000         8.910       0.000             0.000   \n",
       "50%        6.070      0.000         8.910       0.000             0.000   \n",
       "75%        8.000      0.000         8.910       0.130            64.800   \n",
       "max      130.940      1.000        10.000       1.000          2116.800   \n",
       "\n",
       "       Support Requests  Last visited minus Purchase date    Targets  \n",
       "count         14084.000                         14084.000  14084.000  \n",
       "mean              0.070                            61.935      0.159  \n",
       "std               0.472                            88.208      0.366  \n",
       "min               0.000                             0.000      0.000  \n",
       "25%               0.000                             0.000      0.000  \n",
       "50%               0.000                            11.000      0.000  \n",
       "75%               0.000                           105.000      0.000  \n",
       "max              30.000                           464.000      1.000  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.iloc[:,1:].describe().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisión del dataframe\n",
    "\n",
    "Se explica brevemente qué representan algunas de las columnas.\n",
    "\n",
    "- *Book length*: representa la duración en minutos de un libro.\n",
    "- *Price overall*: precio pagado en dólares.\n",
    "- *Review*: En el caso de que *Review* sea igual a 1, el cliente dió un puntaje a su compra. Este puntaje puede ir de 1 a 10.\n",
    "- *Minutes listened*: indica la cantidad de tiempo que la persona escuchó el audiolibro.\n",
    "- *Completion*: porcentaje que indica que tanto fue escuchado el libro respecto del total (en minutos).\n",
    "- *Last visited minus Purchase date*: hace referencia a la cantidad de minutos que la persona entró al sitio desde su compra. Es esperable que mientras más grande sea este número, mayor sea la chance de que la persona compre.\n",
    "\n",
    "Antes de explicar la última columna, se debe aclarar que el set de datos que estamos usando recopila información de clientes en un lapso de tiempo de 2 años y seis meses. Los primeros dos años forman las columnas que van desde *Book length (mins)_overall* hasta *Last visited minus Purchase date*. Los otros seis meses se utilizaron para formar la columna *Targets*.\n",
    "\n",
    "- *Targets*: Esta columna contiene 0 y 1. Un cero indica que el cliente NO compro en el lapso de seis meses, y un uno indcia que sí lo hizo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Completando valores cero\n",
    "\n",
    "La columna *Review 10/10* posee muchos ceros. Esto es normal, en general ninguna persona deja un review. No obstante, debemos tomar alguna medida para no dejar tantos ceros en nuestros datos. Lo que vamos a hacer es rellenar dichos valores con el promedio de la columna, el cual es $8.91$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df[raw_df[\"Review 10/10\"] == 0] = raw_df[\"Review 10/10\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanceando el set de datos\n",
    "\n",
    "Como siempre, debemos analizar los datos que tenemos para saber si están desbalanceados. Analicemos la columna *Targets* para ver que tan desbalanceado esta el set de datos considerando personas que sí compraron libros vs las que no han comprado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.88327179778472"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df[\"Targets\"].sum()/raw_df[\"Targets\"].shape[0]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que tenemos sólo un 15% de personas que hayan comprado libros (también vemos esto en la tabla *describe* del dataframe). ¿Por qué es esto importante? Porque un set de datos desbalanceado provocará que nuestra red neuronal (y cualquier otro algoritmo de ML) tenga un sesgo, ya que rápidamente interpretará que la clase importante es la clase 0, es decir, la gente que no compra libros.\n",
    "\n",
    "Por lo tanto, vamos a balancear los datos. Pero antes, vamos a quedarnos con las columnas que harán las veces de *features* y la columna que hará de *target*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a balancear los datos. Lo que vamos a hacer es la técnica de *downsampling*, esto es, vamos a retirar algunas observaciones de la clase 0 (mayoritaria). Utilizaremos el método [Resample](https://scikit-learn.org/stable/modules/generated/sklearn.utils.resample.html) de Scikitlearn.\n",
    "\n",
    "Info [acá](https://elitedatascience.com/imbalanced-classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    2237\n",
       "1.0    2237\n",
       "Name: Targets, dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clase_mayoritaria = shuffle(raw_df[raw_df[\"Targets\"] == 0], random_state = 42) #mezclamos datos\n",
    "clase_minoritaria = raw_df[raw_df[\"Targets\"] == 1] #no hace falta mezclarlos\n",
    "\n",
    "targets_uno = int(raw_df[raw_df[\"Targets\"] == 1][\"Targets\"].sum()) #cantidad de targets con valor 1\n",
    "\n",
    "# Aplicamos el downsampling\n",
    "clase_mayoritaria_downsampled = resample(clase_mayoritaria, \n",
    "                                 replace=False,    # Sin reemplazo\n",
    "                                 n_samples = targets_uno,     # Cantidad de muestras que queremos.\n",
    "                                 random_state=42) # Seteamos la semilla para tener reproducibilidad en un futuro\n",
    "\n",
    "df_balanceado = pd.concat([clase_mayoritaria_downsampled,clase_minoritaria])\n",
    "df_balanceado[\"Targets\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que ahora el set de datos está balanceado. Lo malo es que hemos perdido muestras, pero es el costo que tenemos que pagar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separando variables independientes y variable dependiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"Book length (mins)_overall\",\"Book length (mins)_avg\", \"Price_overall\", \"Price_avg\", \"Review\", \"Review 10/10\",\n",
    "            \"Completion\", \"Minutes Listened\", \"Support Requests\", \"Last visited minus Purchase date\"]\n",
    "\n",
    "inputData = df_balanceado[features].values #no usamos la columna ID ya que no aporta nada\n",
    "# inputData = df_balanceado[:,1:-1] #forma equivalente\n",
    "\n",
    "targets = df_balanceado[\"Targets\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estandarizando datos\n",
    "\n",
    "Como hemos mencionado, es importante dentro del preprocesamiento hacer que los datos estén estandarizados, es decir, lograr que sus valores máximos y mínimos estén en un rango acotado y al mismo tiempo lograr que tengan media cero y desvío estándar 1 (o cercano a estos valores). Usaremos el método [standardscaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn-preprocessing-standardscaler)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book length (mins)_overall</th>\n",
       "      <th>Book length (mins)_avg</th>\n",
       "      <th>Price_overall</th>\n",
       "      <th>Price_avg</th>\n",
       "      <th>Review</th>\n",
       "      <th>Review 10/10</th>\n",
       "      <th>Completion</th>\n",
       "      <th>Minutes Listened</th>\n",
       "      <th>Support Requests</th>\n",
       "      <th>Last visited minus Purchase date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4474.00</td>\n",
       "      <td>4474.00</td>\n",
       "      <td>4474.00</td>\n",
       "      <td>4474.00</td>\n",
       "      <td>4474.00</td>\n",
       "      <td>4474.00</td>\n",
       "      <td>4474.00</td>\n",
       "      <td>4474.00</td>\n",
       "      <td>4474.00</td>\n",
       "      <td>4474.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.69</td>\n",
       "      <td>-1.85</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>-0.69</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>-11.83</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.74</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.21</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.21</td>\n",
       "      <td>5.94</td>\n",
       "      <td>18.16</td>\n",
       "      <td>14.92</td>\n",
       "      <td>2.27</td>\n",
       "      <td>1.61</td>\n",
       "      <td>4.64</td>\n",
       "      <td>7.81</td>\n",
       "      <td>19.42</td>\n",
       "      <td>3.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Book length (mins)_overall  Book length (mins)_avg  Price_overall  \\\n",
       "count                     4474.00                 4474.00        4474.00   \n",
       "mean                        -0.00                   -0.00          -0.00   \n",
       "std                          1.00                    1.00           1.00   \n",
       "min                         -2.69                   -1.85          -0.63   \n",
       "25%                         -0.74                   -0.74          -0.35   \n",
       "50%                          0.13                   -0.24          -0.23   \n",
       "75%                          1.21                    0.37           0.15   \n",
       "max                          1.21                    5.94          18.16   \n",
       "\n",
       "       Price_avg   Review  Review 10/10  Completion  Minutes Listened  \\\n",
       "count    4474.00  4474.00       4474.00     4474.00           4474.00   \n",
       "mean       -0.00    -0.00         -0.00       -0.00              0.00   \n",
       "std         1.00     1.00          1.00        1.00              1.00   \n",
       "min        -0.69    -0.44        -11.83       -0.39             -0.45   \n",
       "25%        -0.47    -0.44         -0.01       -0.39             -0.45   \n",
       "50%        -0.27    -0.44         -0.01       -0.39             -0.45   \n",
       "75%         0.04    -0.44         -0.01       -0.39             -0.19   \n",
       "max        14.92     2.27          1.61        4.64              7.81   \n",
       "\n",
       "       Support Requests  Last visited minus Purchase date  \n",
       "count           4474.00                           4474.00  \n",
       "mean              -0.00                              0.00  \n",
       "std                1.00                              1.00  \n",
       "min               -0.20                             -0.77  \n",
       "25%               -0.20                             -0.77  \n",
       "50%               -0.20                             -0.55  \n",
       "75%               -0.20                              0.65  \n",
       "max               19.42                              3.43  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "inputData_std = scaler.fit_transform(inputData)\n",
    "inputData_std = pd.DataFrame(inputData_std, columns = [\"Book length (mins)_overall\",\"Book length (mins)_avg\",\n",
    "                                                        \"Price_overall\", \"Price_avg\", \"Review\", \"Review 10/10\",\n",
    "                                                        \"Completion\", \"Minutes Listened\", \"Support Requests\",\n",
    "                                                        \"Last visited minus Purchase date\"])\n",
    "inputData_std.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separando datos en set de entrenamiento, validación y testeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "tarin = 0.8\n",
    "validation = 0.1\n",
    "test = 0.1\n",
    "\n",
    "#Primero separo en set de entrenamiento y de testeo\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(inputData_std.values, targets, test_size = 1 - tarin, random_state = 42)\n",
    "\n",
    "#Ahora separo el set de testeo en sets de validación y de testeo\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size = test/(test + validation), random_state = 42) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guardando los sets y el escalor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomentar las lineas debajo para guardar los sets\n",
    "\n",
    "# np.savez('datos_entrenamiento', inputs = x_train, target = y_train)\n",
    "# np.savez('datos_validación', inputs = x_val, targets = y_val)\n",
    "# np.savez('datos_testeo', inputs = x_test, targets = y_test)\n",
    "\n",
    "# pickle.dump(scaler, open(\"scaler.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problema\n",
    "\n",
    "Tenemos un set de datos que como sabemos, contiene información de clientes con relación a compra de audiolibros. Cada cliente dentro del set de datos ha realizado *al menos una compra*. Queremos crear un algoritmo de ML con los datos disponibles que nos permita predecir si un cliente va a comprar o no un audilibro de la empresa que estamos analizando.\n",
    "\n",
    "La idea básica es ver si la probabilidad de que un cliente *regrese* es baja, entonces no hay razón para gastar dinero en publicidades sobre esa persona. Queremos enfocarnos sólo en aquellos clientes en los cuales tenemos probabilidades de que vuelva a comprar.\n",
    "\n",
    "Recordemos que los datos que tenemos corresponden a un período de dos años y seis meses. Por otro lado, la columna *Targets* es del tipo booleana. Un $0$ representa una persona que no compró y un $1$ indica que la persona sí compró en los últimos 6 meses. Por lo tanto, lo que vamos a hacer es predecir si una persona comprará o no un audio libro considerando $2$ años de actividad de la misma.\n",
    "\n",
    "Como sabemos, este es un problema de *clasificación* ya que tendremos dos estados,\n",
    "\n",
    "- La persona *comprará*.\n",
    "- La persona *no comprará*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creamos modelo\n",
    "\n",
    "Vamos a utilizar la clase [Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) de Keras para crear una red neuronal de 4 layers.\n",
    "\n",
    "- La primera será la layer de entrada. Debido a que tenemos un set de datos con 10 features, la capa de entrada será de diez nodos.\n",
    "- Las segunda y tercera capa serán de 50 nodos cada una.\n",
    "- La última será la capa de salida, que sólo tendrá dos nodos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrada = 10\n",
    "capas_salida = 2\n",
    "capa_oculta = 50\n",
    "    \n",
    "modelo_nn = tf.keras.Sequential([\n",
    "            # Las capaz densas básicamente hacen la operación de producto punto entre los pesos de la red y lo que llega a cada nodo,\n",
    "            # es decir salida = activación(dot(entrada, pesos) + bias)\n",
    "            tf.keras.layers.Dense(capa_oculta, activation='relu'), # capa oculta 1\n",
    "            tf.keras.layers.Dense(capa_oculta, activation='relu'), # capa oculta 2\n",
    "            # Importante: Dado que queremos obtener probabilidades, vamos a usar la función de activación \"SOFTMAX\"\n",
    "            # en la capa de salida\n",
    "            tf.keras.layers.Dense(capas_salida, activation='softmax') # output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seteamos parámetros para entrenamiento del modelo\n",
    "\n",
    "- Optimizador: La función utilizada como optimizador es [ADAM](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#compile), el cual es un algoritmo del tipo *gradiente descendente*\n",
    "- Función de costo (loss): Utilizaremos *entropía cruzada* - [Sparse Categorical Crossentropy](https://keras.io/api/losses/probabilistic_losses/#categoricalcrossentropy-class) la cual es útil cuando tenemos más de una label de salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "36/36 - 1s - loss: 0.5653 - accuracy: 0.7309 - val_loss: 0.5130 - val_accuracy: 0.7629 - 1s/epoch - 33ms/step\n",
      "Epoch 2/100\n",
      "36/36 - 0s - loss: 0.4659 - accuracy: 0.7706 - val_loss: 0.4576 - val_accuracy: 0.7562 - 102ms/epoch - 3ms/step\n",
      "Epoch 3/100\n",
      "36/36 - 0s - loss: 0.4268 - accuracy: 0.7793 - val_loss: 0.4369 - val_accuracy: 0.7763 - 97ms/epoch - 3ms/step\n",
      "Epoch 4/100\n",
      "36/36 - 0s - loss: 0.4059 - accuracy: 0.7868 - val_loss: 0.4281 - val_accuracy: 0.7875 - 97ms/epoch - 3ms/step\n",
      "Epoch 5/100\n",
      "36/36 - 0s - loss: 0.3925 - accuracy: 0.7932 - val_loss: 0.4130 - val_accuracy: 0.7830 - 113ms/epoch - 3ms/step\n",
      "Epoch 6/100\n",
      "36/36 - 0s - loss: 0.3864 - accuracy: 0.7944 - val_loss: 0.4160 - val_accuracy: 0.8031 - 188ms/epoch - 5ms/step\n",
      "Epoch 7/100\n",
      "36/36 - 0s - loss: 0.3802 - accuracy: 0.7988 - val_loss: 0.4025 - val_accuracy: 0.7919 - 128ms/epoch - 4ms/step\n",
      "Epoch 8/100\n",
      "36/36 - 0s - loss: 0.3734 - accuracy: 0.8075 - val_loss: 0.4039 - val_accuracy: 0.7852 - 91ms/epoch - 3ms/step\n",
      "Epoch 9/100\n",
      "36/36 - 0s - loss: 0.3705 - accuracy: 0.8030 - val_loss: 0.4046 - val_accuracy: 0.7785 - 113ms/epoch - 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aa230f4760>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Seleccionamos optimizador y la función de costo a utilizar\n",
    "\n",
    "modelo_nn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# tamaño del lote\n",
    "batch_size = 100\n",
    "\n",
    "# máxima cantidad de épocas para entrenar la red\n",
    "max_epochs = 100\n",
    "\n",
    "# Mecanismo de \"frenado temprano\"\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n",
    "\n",
    "# fit the model\n",
    "# note that this time the train, validation and test data are not iterable\n",
    "modelo_nn.fit(x_train, y_train,\n",
    "              batch_size = batch_size,\n",
    "              epochs = max_epochs, # épocas de entrenamiento\n",
    "              # callbacks are functions called by a task when a task is completed\n",
    "              # Las callbacks son funciones que se llaman cuando una época se completa\n",
    "              # La tarea ahora es chequear un incremento en el valor de la función de costo\n",
    "              callbacks = [early_stopping], # frenado\n",
    "              validation_data = (x_val, y_val), # datos de validación\n",
    "              verbose = 2\n",
    "          )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testeando el modelo\n",
    "\n",
    "Podemos ver casi un 81% de accuracy sobre los datos de validación. ¡No está nada mal!\n",
    "\n",
    "Ahora bien, siempre debemos chequear el modelo **sobre los datos de testeo**, los cuales, se supone, **nunca fueron vistos por el modelo**. Es importante mantener las *seeds* a la hora de generar los set de entrenamiento, validación y testeo para tener reproducibilidad.\n",
    "\n",
    "Uno de los objetivos de probar el modelo sobre el set de testeo es saber si nuestro modelo posee overfitting sobre los datos de validación. Nunca debemos ajustar el modelo (hiperparámetros) sobre el set de testeo.\n",
    "\n",
    "Probemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3873 - accuracy: 0.7924\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = modelo_nn.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test loss: 0.39. Acuracy: 79.24%\n"
     ]
    }
   ],
   "source": [
    "# Imprimimos resultados\n",
    "print('\\nTest loss: {0:.2f}. Acuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es común que el accuracy del modelo de más bajo sobre el set de testeo que sobre el set de validación. No obstante, en nuestro caso, vemos que el modelo funciona un poco mejor (muy levemente) sobre los datos de testeo que sobre los de validación, no obstante, esto esto es casual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obteniendo probabilidades de que un cliente compre\n",
    "\n",
    "Con el modelo anterior, podemos obtener una salida del tipo *comprará* o *no comprará*.\n",
    "\n",
    "No obstante, podríamos estar interesados en obtener las *probabilidades* de compra para poder realizar otros cálculos (similares a los que hicimos con los regresores logísticos).\n",
    "\n",
    "Esto lo hacemos con el método *predicr()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.36, 0.64],\n",
       "       [1.  , 0.  ],\n",
       "       [0.56, 0.44],\n",
       "       [0.  , 1.  ],\n",
       "       [1.  , 0.  ],\n",
       "       [0.58, 0.42],\n",
       "       [0.45, 0.55],\n",
       "       [0.34, 0.66],\n",
       "       [0.35, 0.65],\n",
       "       [1.  , 0.  ]], dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo_nn.predict(x_test).round(2)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los diez primeros datos podemos ver que para el cliente número 1 el modelo predice un $37\\%$ de que no comprará y un $63\\%$ de que sí comprará. Para el cliente 2 tenemos un $100\\%$ de que no comprará, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buscando la máxima probabildiad\n",
    "\n",
    "Es común tener redes neuronales (y otros algoritmos), como en este caso, que nos den más de una probabilidad (*multioutput*). En el ejemplo que estamos trabajando, la red neuronal nos ofrece dos probabilidades, una para la *no compra* y otra para la *compra*, no obstante, podríamos tener mayor cantidad de probabilidades asociadas a diferentes eventos. A su vez, en general queremos buscar la máxima probabilidad de todos esos eventos para cada observación y así saber cual de todos los eventos es el más probable.\n",
    "\n",
    "Python nos ofrece una forma sencilla de encontrar el máximo índice dentro de un numpy.array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = np.array([\n",
    "                [0.1,0.2,0.8],\n",
    "                [0.5,0.25,0.25],\n",
    "                [0.1,0.8,0.1]])\n",
    "\n",
    "np.argmax(temp, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos el enfoque anterior a las probabilidades obtenidas por la red neuronal para los datos de testeo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(modelo_nn.predict(x_test),axis=1)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A modo de ejemplo, la línea de código nos devuelve para el primer cliente el índice 1, indicando que la máxima probabilidad se encuentra en la columna correspondiente a que el cliente *sí comprará* un nuevo audio libro. Por otro lado, para el cliente dos, tenemos que el índice es 0, es decir, que python nos devolvió la columna correspondiente a *no comprará*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardando modelo\n",
    "\n",
    "Vamos a guardar el modelo para usarle en un futuro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_nn.save('modelo_nn_Audilibros.h5') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:costumeranalytics]",
   "language": "python",
   "name": "conda-env-costumeranalytics-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "a04268169029a6c8c242914fc5adae46e7d2251e916247388ae8472995069ae4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
